{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "In this assignment you will learn to build *Bag Of Words* and *N-Gram* Models from scratch. Then you will also implement them using Scikit Learn's Library. At the end of this assignment you will have your own *Sarcasm Detector*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Libraries Here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is the **News Headline Dataset for Sarcasm Detection**. It contains the binary lables for each news headline, such as:\n",
    "\n",
    "* 0 -> No Sarcasm\n",
    "* 1 -> Sarcasm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with the dataset with an **80-20 train-test** split, in the `train.csv` and `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (21367, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shares of hazmat-suit maker spike on nyc ebola...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miss america called before u.n. council for no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tarsiers, the world's smallest primate: animal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'richie rich' comics introduces new, even gaye...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no one able to tell clam just had stroke</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  is_sarcastic\n",
       "0  shares of hazmat-suit maker spike on nyc ebola...             0\n",
       "1  miss america called before u.n. council for no...             1\n",
       "2  tarsiers, the world's smallest primate: animal...             0\n",
       "3  'richie rich' comics introduces new, even gaye...             1\n",
       "4           no one able to tell clam just had stroke             1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "print(\"Shape:\", train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will separate the data and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data[\"headlines\"] # Data that we will pre-process\n",
    "train_Y = train_data[\"is_sarcastic\"] # True labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering + Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job is to use your **Feature Engineering Skills** to convert the text into a feature vector. For this purpose we will first build our `Bag Of Words` and `N-Gram` models. But first we need to make our text uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step while dealing with the natural language is `Text Normalization`, which is done in the following three steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 1:** Write a function `remove_punctuation_marks` that takes a text and replaces all punctuation marks with a single space. (3)\n",
    "\n",
    "**Example:** Let's say we have a text, `\"Are you ready to Co-ordinate with US?\"`. The function must return this text `\"Are you ready to Co ordinate with US \"`\n",
    "\n",
    "*Hint: You can use a regex to detect anything that is not a word.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are you ready to Co ordinate with US '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation_marks(text):\n",
    "    mystring = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return mystring\n",
    "remove_punctuation_marks(\"Are you ready to Co-ordinate with US?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 2:** Write a function `to_lower_case` that takes a text and converts it into lower-case. (2)\n",
    "\n",
    "**Example:** Now the above processed text, `\"Are you ready to Co ordinate with US \"` must become `\"are you ready to co ordinate with us \"`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 3:** Write a function `remove_stop_words` that takes a text and a list of stop words, and removes all of the words that are not in the file of stop words provided to you (i.e. `stopwords.txt`). (5)\n",
    "\n",
    "**Example:** This function will convert the above text, `\"are you ready to co ordinate with us \"` into `\"ready co ordinate with us\"`. Don't forget to remove spaces at the end.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"stopwords.txt\", \"r\")\n",
    "stop_words = file.read().split()\n",
    "file.close()\n",
    "\n",
    "def remove_stop_words(text, stopwords_list):\n",
    "    text_split = text.split(\" \")\n",
    "    word_str = ''\n",
    "    for w in text_split:  \n",
    "        if w not in stopwords_list:  \n",
    "            word_str = word_str + ' ' + w\n",
    "    return word_str.lstrip().rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Bag of Words Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 4:** You are given a function `create_BOW_vocab` that takes a training data and a list of stop words, and returns a Bag Of Words in the form of a `list`. You task is to normalize every text using above defined three functions in the given order and use the property of the `set` to store all unique words. (3)\n",
    "\n",
    "**Example:** Let's say this is out training data:\n",
    "\n",
    "`[ \"Are you ready to Co-ordinate with US?\",\n",
    "   \"We are ready for a race.\"              ]`\n",
    "\n",
    "The function will firstly normalize the text and then it will return all uniques words as `['co', 'ordinate', 'race', 'ready', 'us']`. The Bag of Words does not contain any duplicate words.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_BOW_vocab(train_X, stopwords_list):\n",
    "    bow = set()\n",
    "    strarr = []\n",
    "    for text in train_X:\n",
    "        mystring = remove_punctuation_marks(text)\n",
    "        mystring = to_lower_case(mystring)\n",
    "        mystring = remove_stop_words(mystring, stopwords_list)\n",
    "        strarr = mystring.split(\" \")\n",
    "        for w in strarr:\n",
    "            bow.add(w)\n",
    "    return sorted(list(bow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 5:** You are given a function `BOW_feature_vectors` that takes a training data and a Bag of Words vocabulary list, and return the feature vectors of the training data, in the form of `numpy arrays`. Your job is to normalize every `text` using the three functions we build and then convert it into its vector based upon its features. You may need to revise BOW lecture for this. (3)\n",
    "\n",
    "**Example:** Let's say this is out training data:\n",
    "\n",
    "`[ \"Are you ready to Co-ordinate with US?\",\n",
    "   \"We are ready for a race.\"              ]`\n",
    "\n",
    "and Bag of Words Vocab is `['co', 'ordinate', 'race', 'ready', 'us']`\n",
    "\n",
    "The function will first normalize the text and return the following feature vectors:\n",
    "\n",
    "`[  [1, 1, 0, 1, 1],\n",
    "    [0, 0, 1, 1, 0]  ]`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def BOW_feature_vectors(train_X, BOW):\n",
    "    feature_vectors = list()\n",
    "    strarr = []\n",
    "    for text in train_X:\n",
    "        temp = list()\n",
    "        mystring = remove_punctuation_marks(text)\n",
    "        mystring = to_lower_case(mystring)\n",
    "        mystring = remove_stop_words(mystring, stop_words)\n",
    "        strarr = mystring.split(\" \")\n",
    "        for w in BOW:  \n",
    "            if w not in strarr:\n",
    "                temp.append(0)\n",
    "            else:\n",
    "                temp.append(1)\n",
    "        feature_vectors.append(temp)\n",
    "    return np.array(feature_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) N-Gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 6:** You are given a function `create_NGram_vocab` that takes a training data, a parameter `n`, a list of stop words, and returns an N-Gram vocab in the form of a `list`. Your task is to normalize every text using above defined three functions in the given order and use the property of the `set` to store all unique n-grams. (3)\n",
    "\n",
    "**Example:** Let's say this is out training data:\n",
    "\n",
    "`[ \"Are you ready to Co-ordinate with US?\",\n",
    "   \"We are ready for a race.\"               ]`\n",
    "\n",
    "After normalization the text will look like this:\n",
    "\n",
    "`[ \"ready co ordinate us\",\n",
    "   \"ready race.\"           ]`\n",
    "\n",
    "Now the function will return all unique N-Grams as `['co ordinate', 'ordinate us', 'ready co', 'ready race']`. The N-Gram does not contain any duplicate words.\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NGram_vocab(train_X, n, stopwords_list):\n",
    "    ngram = set()\n",
    "    strarr = []\n",
    "    for text in train_X:\n",
    "        temp = []\n",
    "        mystring = remove_punctuation_marks(text)\n",
    "        mystring = to_lower_case(mystring)\n",
    "        mystring = remove_stop_words(mystring, stop_words)\n",
    "        strarr = mystring.split(\" \")\n",
    "        for x in range(len(strarr)-n+1):\n",
    "            word = \"\"\n",
    "            for y in range(n):\n",
    "                word = word + \" \" + strarr[x+y]\n",
    "            temp.append(word.lstrip())\n",
    "        for x in temp:\n",
    "            ngram.add(x)\n",
    "    return sorted(list(ngram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 7:** You are given a function `NGram_feature_vectors` that takes a training data and an NGram vocabulary/features list, and returns the feature vectors of the training data, in the form of `numpy arrays`. Your job is to normalize every `text` using the three functions we build and then convert it into its vector based upon its features. You may need to revise N-Gram lecture for this. (3)\n",
    "\n",
    "**Example:** Let's say this is out training data:\n",
    "\n",
    "`[ \"Are you ready to Co-ordinate with US?\",\n",
    "   \"We are ready for a race.\"              ]`\n",
    "\n",
    "and Bag of Words Vocab is `['co ordinate', 'ordinate us', 'ready co', 'ready race']`\n",
    "\n",
    "The function will first normalize the text and return the following feature vectors:\n",
    "\n",
    "`[  [1, 1, 1, 0],\n",
    "    [0, 0, 0, 1]  ]`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NGram_feature_vectors(train_X, n, NGram):\n",
    "    feature_vectors = list()\n",
    "    for text in train_X:\n",
    "        strarr = []\n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        ngram = set()\n",
    "        mystring = remove_punctuation_marks(text)\n",
    "        mystring = to_lower_case(mystring)\n",
    "        mystring = remove_stop_words(mystring, stop_words)\n",
    "        strarr = mystring.split(\" \")\n",
    "        for x in range(len(strarr)-n+1):\n",
    "            word = \"\"\n",
    "            for y in range(n):\n",
    "                word = word + \" \" + strarr[x+y]\n",
    "            temp.append(word.lstrip())\n",
    "        for x in temp:\n",
    "            ngram.add(x)\n",
    "        for w in NGram:\n",
    "            if w in ngram:\n",
    "                temp2.append(1)\n",
    "            else:\n",
    "                temp2.append(0)\n",
    "        feature_vectors.append(temp2)\n",
    "    return np.array(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Congragulations!*** You have successfully implemented BOW and N-Gram models by yourself. Now is the time to introduce you to your new best friend, the [Scikit-Learn CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), that you can use to carry out the feature extraction. It has an in-built feature for n-gram vectorization as well. Do check out the documentation to figure out how to use simple Bag of Words vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) BOW using Scikit-learn CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 8:** Read the documentation for CountVector and extract the BOW features. (4)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8\n",
    "points: 4\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_vectorizer = CountVectorizer(stop_words= stop_words, analyzer=\"word\") # Choose the right arguments for CountVectorizer\n",
    "BOW_features = BOW_vectorizer.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### 5) N-Gram using Scikit-learn CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 9:** Read the documentation for CountVector and extract the NGrams features. (4)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q9\n",
    "points: 4\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), stop_words = stop_words) # Choose the right arguments for CountVectorizer\n",
    "NGram_features = NGram_vectorizer.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will be using the [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV) library from SKlearn (this one uses in-built cross validation as well). This part is open-ended and meant for you to explore how to change hyperparameters to get a good result. The coding here is simple - the only job you have to do is look at the documentation. *Data Science is about finding the right libraries to do the job, and again, the coding is simple, your job is to find the right functions.* And the code here is just 2 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5342, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top official resigns from trump epa with scath...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lea delaria gets candid about her wild tour da...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who declares sierra leone free of ebola</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>top of mt. everest pulling away majority of ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sonoma sheriff battles with ice over misinform...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  is_sarcastic\n",
       "0  top official resigns from trump epa with scath...             0\n",
       "1  lea delaria gets candid about her wild tour da...             0\n",
       "2            who declares sierra leone free of ebola             0\n",
       "3  top of mt. everest pulling away majority of ho...             1\n",
       "4  sonoma sheriff battles with ice over misinform...             0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "print(\"Shape:\" , test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_data[\"headlines\"] # Test Data\n",
    "test_Y = test_data[\"is_sarcastic\"] # True Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Using your BOW Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 10:** Read the documentation for Logistic Regression and train a BOW classifier. (5)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q10\n",
    "points: 5\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BOW_classifier = LogisticRegressionCV(max_iter = 500, cv=5, random_state=0).fit(BOW_features, train_Y)\n",
    "X_features = BOW_vectorizer.transform(test_X)\n",
    "bow_array = BOW_classifier.predict(X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### 2) Using your N-Gram Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 11:** Read the documentation for Logistic Regression and train an N-Gram classifier. (5)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q11\n",
    "points: 5\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGram_classifier = LogisticRegressionCV(max_iter = 500, cv=5, random_state=0).fit(NGram_features, train_Y)\n",
    "X_features = NGram_vectorizer.transform(test_X)\n",
    "ngram_array = NGram_classifier.predict(X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) and [confusion_matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py) to get **accuracies** of your classifiers and to **plot confusion matricies** to see how good your models are. You can use the functions provided in the documentation. I'll repeat *Data Science is about finding the right libraries to do the job*, and again, the coding is simple, your job is to find the right functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for BOW is\n",
      "0.8019468363908648\n",
      "\n",
      "Accuracy Score for ngram is\n",
      "0.6664170722575814\n",
      "\n",
      "confusion matrix for BOW is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2554,  615],\n",
       "       [ 443, 1730]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for Ngram is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2831, 1616],\n",
       "       [ 166,  729]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write code to evaluate both models to see which which one performed better\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "bow_score = accuracy_score(bow_array, test_Y)\n",
    "ngram_score = accuracy_score(ngram_array, test_Y)\n",
    "print(\"Accuracy Score for BOW is\")\n",
    "print(bow_score)\n",
    "print(\"\\nAccuracy Score for ngram is\")\n",
    "print(ngram_score)\n",
    "print(\"\\nconfusion matrix for BOW is\")\n",
    "display(confusion_matrix(bow_array, test_Y))\n",
    "print(\"confusion matrix for Ngram is\")\n",
    "display(confusion_matrix(ngram_array, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 12:** Which model was better? Why? Answer in terms of accuracy score and confusion matrix values. (5 (implementation) + 5 (reasoning))\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q12\n",
    "points: 10\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Our results for accuracy score show us clearly that accuracy score for BOW performs better as the value for BOW is ~80% and value for Ngram is ~67%. Our model has a negative bias due to the false negatives we are getting in the Ngram. It also shows that for BOW the wrongly predicted values were lesses in comparission. Through this we can say that the BOW model is performing better here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>q1:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>q2:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>q3:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>q4:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>q5:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>q6:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>q7:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n"
      ],
      "text/plain": [
       "q1:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "q2:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "q3:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "q4:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "q5:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "q6:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "q7:\n",
       "\n",
       "    All tests passed!\n",
       "    \n"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
